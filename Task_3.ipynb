{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Q1**)\n",
        "\n",
        "To use Zero-Shot Learning and Few-Shot Learning for classifying human activities based on accelerometer data, we will use a large language model that can handle text input. Since accelerometer data is not text-based, we'll first convert the accelerometer data into a textual description. then do the zero shot and reconvert them into numbers. Then we can see the predicted labels\n",
        "\n",
        " **1) Zero shot learning**"
      ],
      "metadata": {
        "id": "IAToxXEy5cWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BobTZxYEB02X",
        "outputId": "76c04745-37bb-4376-8a54-5af86f2f4e5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_groq\n",
            "  Downloading langchain_groq-0.1.9-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting groq<1,>=0.4.1 (from langchain_groq)\n",
            "  Downloading groq-0.9.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting langchain-core<0.3.0,>=0.2.26 (from langchain_groq)\n",
            "  Downloading langchain_core-0.2.34-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from groq<1,>=0.4.1->langchain_groq)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.26->langchain_groq) (6.0.2)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.26->langchain_groq)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.75 (from langchain-core<0.3.0,>=0.2.26->langchain_groq)\n",
            "  Downloading langsmith-0.1.101-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.26->langchain_groq) (24.1)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain-core<0.3.0,>=0.2.26->langchain_groq)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.26->langchain_groq)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain_groq)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain_groq) (2.32.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain_groq) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain_groq) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain_groq) (2.0.7)\n",
            "Downloading langchain_groq-0.1.9-py3-none-any.whl (14 kB)\n",
            "Downloading groq-0.9.0-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.2.34-py3-none-any.whl (393 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.9/393.9 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langsmith-0.1.101-py3-none-any.whl (148 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.9/148.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, orjson, jsonpointer, h11, jsonpatch, httpcore, httpx, langsmith, groq, langchain-core, langchain_groq\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed groq-0.9.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.2.34 langchain_groq-0.1.9 langsmith-0.1.101 orjson-3.10.7 tenacity-8.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from langchain_groq.chat_models import ChatGroq\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x_test_path = '/content/X_test.npy'\n",
        "y_test_path = '/content/y_test.npy'\n",
        "x_train_path = '/content/X_train.npy'\n",
        "y_train_path = '/content/y_train.npy'\n",
        "x_test = np.load(x_test_path)\n",
        "y_test = np.load(y_test_path)\n",
        "\n",
        "label_mapping = {\n",
        "    \"LAYING\": 1,\n",
        "    \"SITTING\": 2,\n",
        "    \"STANDING\": 3,\n",
        "    \"WALKING\": 4,\n",
        "    \"WALKING_DOWNWARDS\": 5,\n",
        "    \"WALKING_UPWARDS\": 6\n",
        "}\n",
        "model_name = \"llama3.1-8b\"\n",
        "api_key = \"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\"\n",
        "model = ChatGroq(api_key=api_key)"
      ],
      "metadata": {
        "id": "yZ5TdVGCRtaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to format a single example for zero-shot learning\n",
        "def format_example(example):\n",
        "    ax = example[:, 0]\n",
        "    ay = example[:, 1]\n",
        "    az = example[:, 2]\n",
        "\n",
        "    ax_str = \", \".join(map(str, ax))\n",
        "    ay_str = \", \".join(map(str, ay))\n",
        "    az_str = \", \".join(map(str, az))\n",
        "\n",
        "    # Simplified prompt\n",
        "    formatted_string = (\n",
        "        f\"Classify the activity based on the following accelerometer data:\\n\"\n",
        "        f\"ax: [{ax_str}]\\n\"\n",
        "        f\"ay: [{ay_str}]\\n\"\n",
        "        f\"az: [{az_str}]\\n\"\n",
        "        f\"Activity: \"\n",
        "    )\n",
        "\n",
        "    return formatted_string\n",
        "\n",
        "# Prepare input examples from x_test\n",
        "formatted_examples = [format_example(example) for example in x_test]\n",
        "\n",
        "# Perform zero-shot prediction and print raw predictions\n",
        "y_pred = [model.predict(example) for example in formatted_examples]\n",
        "\n",
        "# Extract activity labels from the model's response\n",
        "def extract_label(response):\n",
        "    match = re.search(r'\\b(LAYING|SITTING|STANDING|WALKING|WALKING_DOWNWARDS|WALKING_UPWARDS)\\b', response.upper())\n",
        "    return match.group(0) if match else \"UNKNOWN\"\n",
        "\n",
        "# Convert predictions to numeric labels\n",
        "y_pred_labels = [label_mapping.get(extract_label(pred), 0) for pred in y_pred]\n",
        "\n",
        "# Print raw predictions, predicted labels, and actual labels for the first 10 samples\n",
        "#print(\"Example\\tRaw Prediction\\tPredicted Label\\tActual Label\")\n",
        "for i in range(5):\n",
        "    raw_pred = y_pred[i]\n",
        "    pred_label = label_mapping.get(extract_label(raw_pred), 0)\n",
        "    actual_label = y_test[i]\n",
        "    #print(f\"{i+1}\\t{raw_pred}\\t{pred_label}\\t{actual_label}\")\n",
        "\n",
        "# Print the numeric predictions for all samples\n",
        "print(\"\\nAll predicted labels:\")\n",
        "print(y_pred_labels)\n",
        "print(\"\\n All True lables:\")\n",
        "print(y_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred_labels)\n",
        "print(f\"\\nAccuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVpYpZmCLPYh",
        "outputId": "13ca8829-4bfa-48a0-ac8d-79615761e177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "All predicted labels:\n",
            "[4, 4, 4, 3, 2, 2, 4, 4, 4, 4, 4, 4, 4, 0, 4, 0, 4, 3, 4, 2, 4, 4, 0, 4, 4, 4, 4, 2, 4, 4, 0, 4, 4, 4, 4, 4, 0, 3, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 0, 2, 4, 3, 4]\n",
            "\n",
            " All True lables:\n",
            "[3 1 2 5 5 1 1 5 3 2 6 5 6 5 6 1 6 5 2 5 4 3 2 2 1 4 6 4 1 2 6 2 4 4 3 6 6\n",
            " 3 1 5 3 2 1 4 4 4 5 1 3 3 3 6 2 4]\n",
            "\n",
            "Accuracy: 0.14814814814814814\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, We get an accuracy around 15% in zero shot learning. For this data. We shall see what happens when we give a few examples and do the same thing."
      ],
      "metadata": {
        "id": "6-XC1h0X_3Fu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) Few shot Learning**"
      ],
      "metadata": {
        "id": "3WpPid235hgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Create a pipeline for zero-shot classification using XLM-R\n",
        "few_shot_classifier = pipeline(\"zero-shot-classification\", model=\"joeddav/xlm-roberta-large-xnli\")\n",
        "\n",
        "# Define a few-shot prompt\n",
        "few_shot_prompt = (\n",
        "    \"Examples of activities with accelerometer data:\\n\"\n",
        "    \"1. ax = 0.1871044, ay = 0.7884942, az = 0.5742789: laying\\n\"\n",
        "    \"2. ax = 0.9908435, ay = -0.10484, az = -0.2516169: sitting\\n\"\n",
        "    \"3. ax = 0.9982275, ay = -0.1632168, az = 0.2099063: standing\\n\"\n",
        "    \"4. ax = 0.6913776, ay = -0.4242498, az = -0.01527766: walking\\n\"\n",
        "    \"5. ax = 0.6580693, ay = -0.26417, az = -0.117143: walking_downwards\\n\"\n",
        "    \"6. ax = 1.240858, ay = -0.1790761, az = -0.5927427: walking_upwards\\n\"\n",
        "    \"\\n\"\n",
        "    \"Classify the following accelerometer data:\\n\"\n",
        "    \"ax = 1.187, ay = -0.6640613, az = -0.5706888\"\n",
        ")\n",
        "\n",
        "# Define candidate labels\n",
        "candidate_labels = [\"laying\", \"sitting\", \"standing\", \"walking\", \"walking_downwards\", \"walking_upwards\"]\n",
        "\n",
        "# Perform few-shot classification\n",
        "result = few_shot_classifier(few_shot_prompt, candidate_labels)\n",
        "\n",
        "# Extract the max score and its corresponding label\n",
        "max_index = result['scores'].index(max(result['scores']))\n",
        "max_score = result['scores'][max_index]\n",
        "max_label = result['labels'][max_index]\n",
        "\n",
        "print(result['labels'])\n",
        "print(result['scores'])\n",
        "print(f\"\\nMax score: {max_score}\\n\")\n",
        "print(f\"Corresponding activity: {max_label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTAe6R9S5j-9",
        "outputId": "b20cbe76-d6ea-425d-90a5-06cf0f0be07d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['walking_upwards', 'laying', 'walking', 'walking_downwards', 'standing', 'sitting']\n",
            "[0.3513725697994232, 0.16650117933750153, 0.16574865579605103, 0.14687234163284302, 0.09441468119621277, 0.07509055733680725]\n",
            "\n",
            "Max score: 0.3513725697994232\n",
            "\n",
            "Corresponding activity: walking_upwards\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output is not correct, but it is very close in saying the output as actual output has 2nd most probability of predicting the output. This is due to the examples we give to few shot learning, it can learn better.\n",
        "\n",
        "Hence Few shots learning is better than Zero shot learning."
      ],
      "metadata": {
        "id": "GdNfNrAeAYlL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3**"
      ],
      "metadata": {
        "id": "GNoDOHT2BkH0"
      }
    }
  ]
}